{"prompt": "<|im_start|>system\nYou are tasked with decomposing a given natural language question into a structured reasoning plan.\n\nEach reasoning plan should consist of multiple sub-questions that are:\n\n- Clear, atomic, and focused on a single retrievable fact.\n- Written as complete, standalone queries that are directly retrievable without ambiguous references.\n- Annotated with a retrieval action type: \"text_retrieval\", \"image_retrieval\", \"general_retrieval\", or \"no_retrieval\".\n- Explicitly connected via dependencies, indicating the logical execution order.\n- If a sub-question relies on the result of another, its query should explicitly reference it (e.g., \"Based on Q1...\").\n\nThe output must be a strictly formatted JSON array, where each element contains:\n\n- name (sub-question identifier, e.g., Q1, Q2, ...)\n- query (the text of the sub-question)\n- action (the retrieval action type)\n- dependencies (a list of sub-question names this sub-question depends on; empty if none)\n\nEnsure that:\n- Each reasoning step logically follows from its dependencies.\n- If a sub-question cannot be answered purely by reasoning and needs external information, it must be annotated with a retrieval action.\n- The final output must be a valid JSON array with no additional explanations, formatting artifacts, or non-JSON text.\n- The entire plan must collectively answer the original query.<|im_end|>\n<|im_start|>user\nWhat role did George Hilton play in \"The Atlantis Interceptors\"?<|im_end|>\n<|im_start|>assistant\n", "predict": "[{\"name\": \"Q1\", \"query\": \"What role did George Hilton play in \\\"The Atlantis Interceptors\\\"?\", \"action\": \"text_retrieval\", \"dependencies\": []}]", "label": "{\"question\": \"What role did George Hilton play in \\\"The Atlantis Interceptors\\\"?\"}\n"}
{"prompt": "<|im_start|>system\nYou are tasked with decomposing a given natural language question into a structured reasoning plan.\n\nEach reasoning plan should consist of multiple sub-questions that are:\n\n- Clear, atomic, and focused on a single retrievable fact.\n- Written as complete, standalone queries that are directly retrievable without ambiguous references.\n- Annotated with a retrieval action type: \"text_retrieval\", \"image_retrieval\", \"general_retrieval\", or \"no_retrieval\".\n- Explicitly connected via dependencies, indicating the logical execution order.\n- If a sub-question relies on the result of another, its query should explicitly reference it (e.g., \"Based on Q1...\").\n\nThe output must be a strictly formatted JSON array, where each element contains:\n\n- name (sub-question identifier, e.g., Q1, Q2, ...)\n- query (the text of the sub-question)\n- action (the retrieval action type)\n- dependencies (a list of sub-question names this sub-question depends on; empty if none)\n\nEnsure that:\n- Each reasoning step logically follows from its dependencies.\n- If a sub-question cannot be answered purely by reasoning and needs external information, it must be annotated with a retrieval action.\n- The final output must be a valid JSON array with no additional explanations, formatting artifacts, or non-JSON text.\n- The entire plan must collectively answer the original query.<|im_end|>\n<|im_start|>user\nWhich Irandhir Santos film was more recent, Elite Squad: The Enemy Within or A Febre do Rato?<|im_end|>\n<|im_start|>assistant\n", "predict": "[{\"name\": \"Q1\", \"query\": \"What is the release year of 'Elite Squad: The Enemy Within'?\", \"action\": \"text_retrieval\", \"dependencies\": []}, {\"name\": \"Q2\", \"query\": \"What is the release year of 'A Febre do Rato'?\", \"action\": \"text_retrieval\", \"dependencies\": []}, {\"name\": \"Q3\", \"query\": \"Which film is more recent: 'Elite Squad: The Enemy Within' (Q1) or 'A Febre do Rato' (Q2)?\", \"action\": \"no_retrieval\", \"dependencies\": [\"Q1\", \"Q2\"]}]", "label": "{\"question\": \"Which Irandhir Santos film was more recent, Elite Squad: The Enemy Within or A Febre do Rato?\"}\n"}
{"prompt": "<|im_start|>system\nYou are tasked with decomposing a given natural language question into a structured reasoning plan.\n\nEach reasoning plan should consist of multiple sub-questions that are:\n\n- Clear, atomic, and focused on a single retrievable fact.\n- Written as complete, standalone queries that are directly retrievable without ambiguous references.\n- Annotated with a retrieval action type: \"text_retrieval\", \"image_retrieval\", \"general_retrieval\", or \"no_retrieval\".\n- Explicitly connected via dependencies, indicating the logical execution order.\n- If a sub-question relies on the result of another, its query should explicitly reference it (e.g., \"Based on Q1...\").\n\nThe output must be a strictly formatted JSON array, where each element contains:\n\n- name (sub-question identifier, e.g., Q1, Q2, ...)\n- query (the text of the sub-question)\n- action (the retrieval action type)\n- dependencies (a list of sub-question names this sub-question depends on; empty if none)\n\nEnsure that:\n- Each reasoning step logically follows from its dependencies.\n- If a sub-question cannot be answered purely by reasoning and needs external information, it must be annotated with a retrieval action.\n- The final output must be a valid JSON array with no additional explanations, formatting artifacts, or non-JSON text.\n- The entire plan must collectively answer the original query.<|im_end|>\n<|im_start|>user\nWhat object is in front of Felicia Day's face?<|im_end|>\n<|im_start|>assistant\n", "predict": "[{\"name\": \"Q1\", \"query\": \"What object is in front of Felicia Day's face?\", \"action\": \"image_retrieval\", \"dependencies\": []}]", "label": "{\"question\": \"What object is in front of Felicia Day's face?\"}\n"}
{"prompt": "<|im_start|>system\nYou are tasked with decomposing a given natural language question into a structured reasoning plan.\n\nEach reasoning plan should consist of multiple sub-questions that are:\n\n- Clear, atomic, and focused on a single retrievable fact.\n- Written as complete, standalone queries that are directly retrievable without ambiguous references.\n- Annotated with a retrieval action type: \"text_retrieval\", \"image_retrieval\", \"general_retrieval\", or \"no_retrieval\".\n- Explicitly connected via dependencies, indicating the logical execution order.\n- If a sub-question relies on the result of another, its query should explicitly reference it (e.g., \"Based on Q1...\").\n\nThe output must be a strictly formatted JSON array, where each element contains:\n\n- name (sub-question identifier, e.g., Q1, Q2, ...)\n- query (the text of the sub-question)\n- action (the retrieval action type)\n- dependencies (a list of sub-question names this sub-question depends on; empty if none)\n\nEnsure that:\n- Each reasoning step logically follows from its dependencies.\n- If a sub-question cannot be answered purely by reasoning and needs external information, it must be annotated with a retrieval action.\n- The final output must be a valid JSON array with no additional explanations, formatting artifacts, or non-JSON text.\n- The entire plan must collectively answer the original query.<|im_end|>\n<|im_start|>user\nWhat dub language was used by Gwen Tennyson in Animated series of Parignya Pandya Shah?<|im_end|>\n<|im_start|>assistant\n", "predict": "[{\"name\": \"Q1\", \"query\": \"What dub language was used by Gwen Tennyson in Animated series of Parignya Pandya Shah?\", \"action\": \"text_retrieval\", \"dependencies\": []}]", "label": "{\"question\": \"What dub language was used by Gwen Tennyson in Animated series of Parignya Pandya Shah?\"}\n"}
